{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from notebooks.ext_imports import *\n",
    "\n",
    "\n",
    "activity = 'Firemaking'\n",
    "user_limit = 2500\n",
    "skill_type = SkillType.EXPERIENCE\n",
    "\n",
    "mlflow.set_experiment(f\"{activity} {SkillType.EXPERIENCE.description} Model comparison for {user_limit} users with specific preprocessor\")  # Set your experiment name\n",
    "\n",
    "df, formatter = get_dataframe(activity, limit=user_limit, aggregate=True, skill_type=skill_type)\n",
    "\n",
    "X = df.drop(columns=['Banned', 'pid'])\n",
    "y = df['Banned']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T13:36:30.644617500Z",
     "start_time": "2024-05-18T13:36:23.868166600Z"
    }
   },
   "id": "e135f0541884ce08",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ee9745822f01bf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T13:36:30.659609900Z",
     "start_time": "2024-05-18T13:36:30.648611300Z"
    }
   },
   "outputs": [],
   "source": [
    "standard_features = formatter.agg_skills + formatter.agg_minigames\n",
    "robust_features = formatter.extra_features\n",
    "minmax_features = formatter.live_skills\n",
    "minmax_features_2 = formatter.live_minigames\n",
    "\n",
    "# Creating the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('std', StandardScaler(), standard_features),\n",
    "    ('robust', RobustScaler(), robust_features),\n",
    "    ('minmax', MinMaxScaler(), minmax_features),\n",
    "    ('minmax_2', MinMaxScaler(), minmax_features_2),\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1745, number of negative: 1745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 3490, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1745, number of negative: 1745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 3490, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000364 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1745, number of negative: 1745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.154551 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 3490, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1745, number of negative: 1745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000418 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 3490, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008892 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000684 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1745, number of negative: 1745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3490, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1745, number of negative: 1745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3490, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000639 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000624 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1745, number of negative: 1745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000493 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3490, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1745, number of negative: 1745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000549 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3490, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000594 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1745, number of negative: 1745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000750 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3490, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1745, number of negative: 1745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3490, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1745, number of negative: 1745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000813 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3490, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1745, number of negative: 1745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3490, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000908 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000747 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1745, number of negative: 1745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3490, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1745, number of negative: 1745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3490, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001104 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1745, number of negative: 1745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3490, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1745, number of negative: 1745\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3490, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1746, number of negative: 1746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10200\n",
      "[LightGBM] [Info] Number of data points in the train set: 3492, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": "            Classifier  PCA_N_COMPONENTS  Accuracy  Recall_0  Recall_1  \\\n15          ExtraTrees                30    0.8840  0.948213  0.487421   \n22          ExtraTrees                40    0.8832  0.948213  0.474843   \n19      LGBMClassifier                30    0.8816  0.929423  0.562893   \n8           ExtraTrees                20    0.8800  0.941338  0.443396   \n27       XGBClassifier                40    0.8800  0.931714  0.534591   \n21        RandomForest                40    0.8796  0.931714  0.512579   \n20       XGBClassifier                30    0.8792  0.927131  0.544025   \n26      LGBMClassifier                40    0.8788  0.930339  0.544025   \n1           ExtraTrees                15    0.8760  0.935839  0.449686   \n14        RandomForest                30    0.8740  0.931714  0.522013   \n12      LGBMClassifier                20    0.8732  0.919340  0.544025   \n7         RandomForest                20    0.8712  0.924840  0.496855   \n0         RandomForest                15    0.8676  0.923006  0.490566   \n13       XGBClassifier                20    0.8664  0.921173  0.547170   \n5       LGBMClassifier                15    0.8624  0.906966  0.500000   \n6        XGBClassifier                15    0.8620  0.910632  0.509434   \n16    GradientBoosting                30    0.8572  0.891384  0.672956   \n23    GradientBoosting                40    0.8568  0.886801  0.650943   \n9     GradientBoosting                20    0.8424  0.870761  0.622642   \n24                 SVM                40    0.8268  0.847388  0.676101   \n2     GradientBoosting                15    0.8260  0.857929  0.613208   \n17                 SVM                30    0.8244  0.843721  0.679245   \n10                 SVM                20    0.8076  0.829973  0.669811   \n18  LogisticRegression                30    0.8048  0.812557  0.726415   \n25  LogisticRegression                40    0.8040  0.820348  0.707547   \n3                  SVM                15    0.7948  0.814849  0.663522   \n11  LogisticRegression                20    0.7808  0.796517  0.694969   \n4   LogisticRegression                15    0.7568  0.768561  0.698113   \n\n    Accuracy_0  Accuracy_1   ROC-AUC                     Matrix  \n15      0.8896      0.8896  0.717817  [[2069, 113], [163, 155]]  \n22      0.8880      0.8880  0.711528  [[2069, 113], [167, 151]]  \n19      0.8828      0.8828  0.746158  [[2028, 154], [139, 179]]  \n8       0.8780      0.8780  0.692367  [[2054, 128], [177, 141]]  \n27      0.8812      0.8812  0.733153  [[2033, 149], [148, 170]]  \n21      0.8784      0.8784  0.722146  [[2033, 149], [155, 163]]  \n20      0.8784      0.8784  0.735578  [[2023, 159], [145, 173]]  \n26      0.8812      0.8812  0.737182  [[2030, 152], [145, 173]]  \n1       0.8740      0.8740  0.692762  [[2042, 140], [175, 143]]  \n14      0.8796      0.8796  0.726863  [[2033, 149], [152, 166]]  \n12      0.8716      0.8716  0.731683  [[2006, 176], [145, 173]]  \n7       0.8704      0.8704  0.710847  [[2018, 164], [160, 158]]  \n0       0.8680      0.8680  0.706786  [[2014, 168], [162, 156]]  \n13      0.8736      0.8736  0.734172  [[2010, 172], [144, 174]]  \n5       0.8552      0.8552  0.703483  [[1979, 203], [159, 159]]  \n6       0.8596      0.8596  0.710033  [[1987, 195], [156, 162]]  \n16      0.8636      0.8636  0.782170  [[1945, 237], [104, 214]]  \n23      0.8568      0.8568  0.768872  [[1935, 247], [111, 207]]  \n9       0.8392      0.8392  0.746701  [[1900, 282], [120, 198]]  \n24      0.8256      0.8256  0.761744  [[1849, 333], [103, 215]]  \n2       0.8268      0.8268  0.735568  [[1872, 310], [123, 195]]  \n17      0.8228      0.8228  0.761483  [[1841, 341], [102, 216]]  \n10      0.8096      0.8096  0.749892  [[1811, 371], [105, 213]]  \n18      0.8016      0.8016  0.769486   [[1773, 409], [87, 231]]  \n25      0.8060      0.8060  0.763948   [[1790, 392], [93, 225]]  \n3       0.7956      0.7956  0.739185  [[1778, 404], [107, 211]]  \n11      0.7836      0.7836  0.745743   [[1738, 444], [97, 221]]  \n4       0.7596      0.7596  0.733337   [[1677, 505], [96, 222]]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Classifier</th>\n      <th>PCA_N_COMPONENTS</th>\n      <th>Accuracy</th>\n      <th>Recall_0</th>\n      <th>Recall_1</th>\n      <th>Accuracy_0</th>\n      <th>Accuracy_1</th>\n      <th>ROC-AUC</th>\n      <th>Matrix</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15</th>\n      <td>ExtraTrees</td>\n      <td>30</td>\n      <td>0.8840</td>\n      <td>0.948213</td>\n      <td>0.487421</td>\n      <td>0.8896</td>\n      <td>0.8896</td>\n      <td>0.717817</td>\n      <td>[[2069, 113], [163, 155]]</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>ExtraTrees</td>\n      <td>40</td>\n      <td>0.8832</td>\n      <td>0.948213</td>\n      <td>0.474843</td>\n      <td>0.8880</td>\n      <td>0.8880</td>\n      <td>0.711528</td>\n      <td>[[2069, 113], [167, 151]]</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>LGBMClassifier</td>\n      <td>30</td>\n      <td>0.8816</td>\n      <td>0.929423</td>\n      <td>0.562893</td>\n      <td>0.8828</td>\n      <td>0.8828</td>\n      <td>0.746158</td>\n      <td>[[2028, 154], [139, 179]]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>ExtraTrees</td>\n      <td>20</td>\n      <td>0.8800</td>\n      <td>0.941338</td>\n      <td>0.443396</td>\n      <td>0.8780</td>\n      <td>0.8780</td>\n      <td>0.692367</td>\n      <td>[[2054, 128], [177, 141]]</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>XGBClassifier</td>\n      <td>40</td>\n      <td>0.8800</td>\n      <td>0.931714</td>\n      <td>0.534591</td>\n      <td>0.8812</td>\n      <td>0.8812</td>\n      <td>0.733153</td>\n      <td>[[2033, 149], [148, 170]]</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>RandomForest</td>\n      <td>40</td>\n      <td>0.8796</td>\n      <td>0.931714</td>\n      <td>0.512579</td>\n      <td>0.8784</td>\n      <td>0.8784</td>\n      <td>0.722146</td>\n      <td>[[2033, 149], [155, 163]]</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>XGBClassifier</td>\n      <td>30</td>\n      <td>0.8792</td>\n      <td>0.927131</td>\n      <td>0.544025</td>\n      <td>0.8784</td>\n      <td>0.8784</td>\n      <td>0.735578</td>\n      <td>[[2023, 159], [145, 173]]</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>LGBMClassifier</td>\n      <td>40</td>\n      <td>0.8788</td>\n      <td>0.930339</td>\n      <td>0.544025</td>\n      <td>0.8812</td>\n      <td>0.8812</td>\n      <td>0.737182</td>\n      <td>[[2030, 152], [145, 173]]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ExtraTrees</td>\n      <td>15</td>\n      <td>0.8760</td>\n      <td>0.935839</td>\n      <td>0.449686</td>\n      <td>0.8740</td>\n      <td>0.8740</td>\n      <td>0.692762</td>\n      <td>[[2042, 140], [175, 143]]</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>RandomForest</td>\n      <td>30</td>\n      <td>0.8740</td>\n      <td>0.931714</td>\n      <td>0.522013</td>\n      <td>0.8796</td>\n      <td>0.8796</td>\n      <td>0.726863</td>\n      <td>[[2033, 149], [152, 166]]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>LGBMClassifier</td>\n      <td>20</td>\n      <td>0.8732</td>\n      <td>0.919340</td>\n      <td>0.544025</td>\n      <td>0.8716</td>\n      <td>0.8716</td>\n      <td>0.731683</td>\n      <td>[[2006, 176], [145, 173]]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>RandomForest</td>\n      <td>20</td>\n      <td>0.8712</td>\n      <td>0.924840</td>\n      <td>0.496855</td>\n      <td>0.8704</td>\n      <td>0.8704</td>\n      <td>0.710847</td>\n      <td>[[2018, 164], [160, 158]]</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>RandomForest</td>\n      <td>15</td>\n      <td>0.8676</td>\n      <td>0.923006</td>\n      <td>0.490566</td>\n      <td>0.8680</td>\n      <td>0.8680</td>\n      <td>0.706786</td>\n      <td>[[2014, 168], [162, 156]]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>XGBClassifier</td>\n      <td>20</td>\n      <td>0.8664</td>\n      <td>0.921173</td>\n      <td>0.547170</td>\n      <td>0.8736</td>\n      <td>0.8736</td>\n      <td>0.734172</td>\n      <td>[[2010, 172], [144, 174]]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>LGBMClassifier</td>\n      <td>15</td>\n      <td>0.8624</td>\n      <td>0.906966</td>\n      <td>0.500000</td>\n      <td>0.8552</td>\n      <td>0.8552</td>\n      <td>0.703483</td>\n      <td>[[1979, 203], [159, 159]]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>XGBClassifier</td>\n      <td>15</td>\n      <td>0.8620</td>\n      <td>0.910632</td>\n      <td>0.509434</td>\n      <td>0.8596</td>\n      <td>0.8596</td>\n      <td>0.710033</td>\n      <td>[[1987, 195], [156, 162]]</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>GradientBoosting</td>\n      <td>30</td>\n      <td>0.8572</td>\n      <td>0.891384</td>\n      <td>0.672956</td>\n      <td>0.8636</td>\n      <td>0.8636</td>\n      <td>0.782170</td>\n      <td>[[1945, 237], [104, 214]]</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>GradientBoosting</td>\n      <td>40</td>\n      <td>0.8568</td>\n      <td>0.886801</td>\n      <td>0.650943</td>\n      <td>0.8568</td>\n      <td>0.8568</td>\n      <td>0.768872</td>\n      <td>[[1935, 247], [111, 207]]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>GradientBoosting</td>\n      <td>20</td>\n      <td>0.8424</td>\n      <td>0.870761</td>\n      <td>0.622642</td>\n      <td>0.8392</td>\n      <td>0.8392</td>\n      <td>0.746701</td>\n      <td>[[1900, 282], [120, 198]]</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>SVM</td>\n      <td>40</td>\n      <td>0.8268</td>\n      <td>0.847388</td>\n      <td>0.676101</td>\n      <td>0.8256</td>\n      <td>0.8256</td>\n      <td>0.761744</td>\n      <td>[[1849, 333], [103, 215]]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GradientBoosting</td>\n      <td>15</td>\n      <td>0.8260</td>\n      <td>0.857929</td>\n      <td>0.613208</td>\n      <td>0.8268</td>\n      <td>0.8268</td>\n      <td>0.735568</td>\n      <td>[[1872, 310], [123, 195]]</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>SVM</td>\n      <td>30</td>\n      <td>0.8244</td>\n      <td>0.843721</td>\n      <td>0.679245</td>\n      <td>0.8228</td>\n      <td>0.8228</td>\n      <td>0.761483</td>\n      <td>[[1841, 341], [102, 216]]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>SVM</td>\n      <td>20</td>\n      <td>0.8076</td>\n      <td>0.829973</td>\n      <td>0.669811</td>\n      <td>0.8096</td>\n      <td>0.8096</td>\n      <td>0.749892</td>\n      <td>[[1811, 371], [105, 213]]</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>LogisticRegression</td>\n      <td>30</td>\n      <td>0.8048</td>\n      <td>0.812557</td>\n      <td>0.726415</td>\n      <td>0.8016</td>\n      <td>0.8016</td>\n      <td>0.769486</td>\n      <td>[[1773, 409], [87, 231]]</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>LogisticRegression</td>\n      <td>40</td>\n      <td>0.8040</td>\n      <td>0.820348</td>\n      <td>0.707547</td>\n      <td>0.8060</td>\n      <td>0.8060</td>\n      <td>0.763948</td>\n      <td>[[1790, 392], [93, 225]]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SVM</td>\n      <td>15</td>\n      <td>0.7948</td>\n      <td>0.814849</td>\n      <td>0.663522</td>\n      <td>0.7956</td>\n      <td>0.7956</td>\n      <td>0.739185</td>\n      <td>[[1778, 404], [107, 211]]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>LogisticRegression</td>\n      <td>20</td>\n      <td>0.7808</td>\n      <td>0.796517</td>\n      <td>0.694969</td>\n      <td>0.7836</td>\n      <td>0.7836</td>\n      <td>0.745743</td>\n      <td>[[1738, 444], [97, 221]]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LogisticRegression</td>\n      <td>15</td>\n      <td>0.7568</td>\n      <td>0.768561</td>\n      <td>0.698113</td>\n      <td>0.7596</td>\n      <td>0.7596</td>\n      <td>0.733337</td>\n      <td>[[1677, 505], [96, 222]]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "N_JOBS = 8\n",
    "# Classifier list\n",
    "classifiers = [\n",
    "    (\"RandomForest\", RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=N_JOBS)),\n",
    "    (\"ExtraTrees\", ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=N_JOBS)),\n",
    "    (\"GradientBoosting\", GradientBoostingClassifier(random_state=42)),  # Does not support n_jobs\n",
    "    (\"SVM\", SVC(probability=True, random_state=42)),  # Does not support n_jobs\n",
    "    (\"LogisticRegression\", LogisticRegression(random_state=42, n_jobs=N_JOBS)),\n",
    "    (\"LGBMClassifier\", LGBMClassifier(random_state=42, n_jobs=16)),\n",
    "    (\"XGBClassifier\", XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=N_JOBS))\n",
    "]\n",
    "\n",
    "\n",
    "# Results DataFrame\n",
    "\n",
    "rows = []\n",
    "\n",
    "PCA_COMPONENTS = [\n",
    "    15,20,30,40\n",
    "]\n",
    "\n",
    "for pca_n_components in PCA_COMPONENTS:\n",
    "    for name, classifier in classifiers:\n",
    "        with mlflow.start_run():\n",
    "        \n",
    "            # Create the pipeline\n",
    "            pipeline = ImblearnPipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('smote', SVMSMOTE(random_state=42)),\n",
    "                ('PCA', PCA(n_components=pca_n_components)),\n",
    "                ('classifier', classifier)\n",
    "            ])\n",
    "            \n",
    "            # Log pipeline components and PCA components\n",
    "            mlflow.log_param(\"PCA_n_components\", pca_n_components)\n",
    "            mlflow.log_param(\"Classifier\", name)\n",
    "            mlflow.log_param(\"Sampling\", \"SVMSMOTE\")\n",
    "            \n",
    "            \n",
    "        \n",
    "            # Calculate scores\n",
    "            accuracy_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
    "            y_pred_proba  = cross_val_predict(pipeline, X, y, cv=cv)\n",
    "            \n",
    "            y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "            accuracy_per_class = [\n",
    "                accuracy_score(y == k, y_pred == k) for k in [0, 1]\n",
    "            ]\n",
    "            roc_auc = roc_auc_score(y, y_pred)\n",
    "            \n",
    "            \n",
    "            recall_per_class = recall_score(y, y_pred, average=None)\n",
    "        \n",
    "            # Confusion matrix\n",
    "            conf_matrix = confusion_matrix(y, y_pred)\n",
    "            \n",
    "            data = {\n",
    "                'Classifier': [name],\n",
    "                'PCA_N_COMPONENTS': pca_n_components,\n",
    "                'Accuracy': [np.mean(accuracy_scores)],\n",
    "                'Recall_0': [recall_per_class[0]],\n",
    "                'Recall_1': [recall_per_class[1]],\n",
    "                'Accuracy_0': [accuracy_per_class[0]],\n",
    "                'Accuracy_1': [accuracy_per_class[1]],\n",
    "                'ROC-AUC': [roc_auc],\n",
    "                'Matrix': [conf_matrix.tolist()]\n",
    "            }\n",
    "            rows.append(\n",
    "                (name,pca_n_components, np.mean(accuracy_scores), \n",
    "                 recall_per_class[0], recall_per_class[1], \n",
    "                 accuracy_per_class[0], accuracy_per_class[1], \n",
    "                 roc_auc, conf_matrix.tolist() )\n",
    "            )\n",
    "            \n",
    "            \n",
    "            \n",
    "            mlflow.log_metric(\"Mean Accuracy\", np.mean(accuracy_scores))\n",
    "            mlflow.log_metric(\"ROC-AUC\", roc_auc)\n",
    "            mlflow.log_metric(\"Recall Class 0\", recall_per_class[0])\n",
    "            mlflow.log_metric(\"Recall Class 1\", recall_per_class[1])\n",
    "            \n",
    "            mlflow.log_metric(\"Accuracy Class 0\", accuracy_per_class[0])\n",
    "            mlflow.log_metric(\"Accuracy Class 1\", accuracy_per_class[1])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            df_conf_matrix = pd.DataFrame(conf_matrix, index=[\"True Neg\", \"True Pos\"], columns=[\"Pred Neg\", \"Pred Pos\"])\n",
    "            conf_matrix_file_path = f\"confusion_matrix_{name}_pca{pca_n_components}.csv\"\n",
    "            df_conf_matrix.to_csv(conf_matrix_file_path)\n",
    "            mlflow.log_artifact(conf_matrix_file_path)\n",
    "            os.remove(conf_matrix_file_path)\n",
    "\n",
    "            \n",
    "            \n",
    "            mlflow.sklearn.log_model(pipeline, \"model\")\n",
    "\n",
    "            mlflow.end_run()\n",
    "    \n",
    "results = pd.DataFrame(rows, columns=['Classifier', 'PCA_N_COMPONENTS', 'Accuracy', 'Recall_0', 'Recall_1', 'Accuracy_0', 'Accuracy_1', 'ROC-AUC', 'Matrix' ])\n",
    "# Print the results sorted by 'Accuracy' and 'Recall'\n",
    "results.sort_values(by=['Accuracy', 'Recall_1'], ascending=False, inplace=True)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T13:46:11.017138300Z",
     "start_time": "2024-05-18T13:36:30.657609900Z"
    }
   },
   "id": "5e82a1985adfbf4c",
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
